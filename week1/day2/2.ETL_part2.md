# Extract

## Different File Locations

### Local files

```
import pandas as pd

df = pd.read_csv("country_data.csv")

```

### Files in a different location

```
import pandas as pd

df = pd.read_csv("data/country_data.csv")

```

### File from a URL

```
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/ChpcTraining/css2024_data/main/country_data.csv")

```

## Different File Types

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/7fbe4893-264e-4140-8d65-81e215fb9d68)


https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html

### Text Files files

Text file with semi-colon

```
df = pd.read_csv("Geospatial Data.txt",sep=";")
```

### Excel

```
df = pd.read_excel("residentdoctors.xlsx")
```

### Json

```
df = pd.read_json("student_data.json")
```

## Other options:

>_Would you like to see the following:_

- Webscraping
  
- Connecting to Databases


# Transform:

As we saw in the previous lesson, sometimes the data is not in the correct format. Now, if you have a one or two files then just rather do it in Excel itself. If you have many files, rather use Python as you can automate the process and most importanly your script records what you did!

There is a something to consider here is that if you don't know your data is in the wrong format or messy, then some of what you will learn now might not make sense. For example, someone might see your table is messy full of books and they recommend you tidy it up so you can work better. And you say, well its not messy at all I work just fine. Fine maybe, but maybe not as efficient and effective as possible. And if your using simulation software or doing cloud computing your data needs to be unambigious.  

So, what I am saying is, if you your use cleaning your data in Excel or other tools then this should be quite familiar to you. Now we are just using Python to do it.

So there are many ways to clean or transform files. Sometimes this is done in the "Extract" process too, so there is an overlap. We will begin by getting familiar with some easy built-in operations or parameters that the the the read_csv function has: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/f8168c97-cd0e-47e5-82f9-ef8636016bce)

Let us look at a few files with some small issues:

## Index Column

```
df = pd.read_csv("country_data.csv")
```

The "Unnamed: 0" column often appears in pandas DataFrames when reading a CSV file if the CSV file itself has an index column that was not explicitly specified.

When you read a CSV file into a DataFrame using pd.read_csv(), pandas assigns an index to each row by default. If the CSV file already contains an index column, pandas will create a new index unless you specify the existing column as the index. If no index is specified, pandas will create a default numerical index, and this default index is labeled as "Unnamed: 0" in the DataFrame.

To avoid the appearance of the "Unnamed: 0" column, you can either specify the existing index column from your CSV file when reading the data, or you can use the index_col parameter to explicitly specify which column you want to use as the index.

```
df = pd.read_csv("country_data.csv",index_col=0)
```

This will read the CSV file and use the first column as the index, preventing the appearance of the "Unnamed: 0" column in your DataFrame.

## Skip Rows

```
df = pd.read_csv("insurance_data.csv")
```

We get an error because the number of columns in your CSV file is not consistent across all rows. The read_csv function in pandas assumes a consistent number of columns, but your data seems to have two values in some rows, causing the error.

We can resolve this by using the "skiprows" parameter to just skip first 5 rows:

```
df = pd.read_csv("insurance_data.csv",skiprows=5)
```

## Column Headings

```
df = pd.read_csv("patient_data.csv")
```

No headings :(

Create a list with the heading names you want and add use the parameters "header=None" and "names=column_names":

```
column_names = ["duration", "pulse", "max_pulse", "calories"]

df = pd.read_csv("patient_data.csv", header=None, names=column_names)
```

## Unique Delimiter

```
df = pd.read_csv("Geospatial Data.txt")
```

This is a text file with the data separate with a semi-colon, Pandas is expecting a comma. So we use the parameter sep=";"

```
df = pd.read_csv("data/Geospatial Data.txt",sep=";")
```

Also note that the file name has a space in it, which is not recommended.

##  Inconsistent Data Types & Names

```
df = pd.read_excel("residentdoctors.xlsx")
```

- Some column headings are snake case and other all caps
- AGEDIST has numbers with text
- MARITULSTATUS has text fields

Note with Excel files, you cannot have it open in Spyder and in Excel too.

Let us first look at the different groupings:

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/aebe6e95-62ef-4ebb-bdb7-f251356d7109)

One way to make this more efficient is to convert it the lower age in grouping. So instead of 30-34yrs it would be 34. This is just one way. You can keep it as is however it will prevent you from doing further numerical analysis on it.

Add the following code:

```
# Step 1: Extract the lower end of the age range (digits only)
df['LOWER_AGE'] = df['AGEDIST'].str.extract('(\d+)-')
```

Pseudo-code:
1. Search for a number followed by a hyphen like "30-"
2. If you find that number, extract the number and ignore the hyphen
3. Put it in a new column called LOWER_AGE 

Then convert that number to from s tring to integer, a whole number.

This step uses "str.extract()" to capture one or more digits from the 'AGEDIST' column. When you use `df['AGEDIST'].str.extract('(\d+)-')`, it applies this regular expression pattern to each element in the 'AGEDIST' column. 

Regular expressions (regex or regexp) are sequences of characters that define a search pattern. It extracts the portion of the string that matches the pattern, which is the one or more consecutive digits before the hyphen in the age range. We won't go over regular expressions much, this was just a demonstration to show you different possibilitles.

```
# Step 2: Convert the new column to float
df['LOWER_AGE'] = df['LOWER_AGE'].astype(int)
```

Pseudo-code:
1. Convert all the data to integers
2. Store that value back into LOWER_AGE column

After extracting the digits, this step converts the resulting column to floating-point numbers using astype(float). Without this conversion, the extracted values would be treated as strings, and any subsequent numeric operations or analysis would not work as expected.

#### Pandas Functions/Methods

In pandas, .str, .extract(), and .astype() are all functions or methods that can be applied to a pandas Series object or single column of text data. 

These methods are part of the powerful functionality provided by pandas for manipulating and analyzing data in a DataFrame. 

When you apply .str to a column, it allows you to perform various string operations on each element of the Series. Here we used .extract() to extract certain information from a string. Other string methods:
- .upper() makes string/text upper case
- .lower() makes string/text lower case
- .replace() replaces certain characters with another
  
*Note, they all have brackets as they can accept parameters.*

## Working with Dates

Again there are many ways to do things, sometimes the recommendations are useful and sometimes they are not. Working with dates and time is a tricky subject, as it all depends what you want to do with it. One general problem with dates in Excel is that formatting can be completely different from one laptop to another, you may send an Excel spreadsheet to someone else and the dates maybe completely different which may effect the analysis you did. For example:

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/f6819af8-d970-4b51-9bcb-db272166fe97)

Ideally you would want to work with a date format that is completely clear for yourself and the next person working with it. Let us look at this time series data:

```
df = pd.read_csv("time_series_data.csv")
```

You will notice the date is in this format: 2020-01-10, however in the US, the day is the middle number and in UK and in most local countries here the middle number is the month - as it should be :)

Another consideration is that dates are usually interpreted as strings or text in a csv file. This can be checked with the .info() method. So the first thing you need to do is convert it to date format using Pandas.

```
# Convert the 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])
```

For example, if your date string is in the "DD-MM-YYYY" format, you would specify the format like this:

```
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')
```

%d: day
%m: month
%Y: year

This not only makes the date column unambiguous but also enables various date-related operations and manipulations. For example:

- filtering by date or range
- calculating time difference
- extracting year, month, day
- add shifts or lags to data
- rolling windows
- timezones
- date arithmetic

Another option is to split the Date column into separate columns, i.e. year, month and day. For example:

```
# Split the 'Date' column into separate columns for year, month, and day
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
```

Output:

```
        Date  Temperature  Year  Month  Day
0 2020-01-01        27.48  2020      1    1
1 2020-01-02        24.31  2020      1    2
2 2020-01-03        28.24  2020      1    3
3 2020-01-04        32.62  2020      1    4
4 2020-01-05        23.83  2020      1    5
5 2020-01-06        23.83  2020      1    6
6 2020-01-07        32.90  2020      1    7
7 2020-01-08        28.84  2020      1    8
...
```

There may be cases when you need to do this for example:

- time series analysis
- seasonal analysis
- grouping and aggregation
- feature engineering for machine learning
- database filtering
- enhance data integrity


## Applying Data Transformations

Now we will look at more involved data transformation methods, namely aggregations, appending, merging, and filtering. We will demonstrate how to:

- aggregate data using groupby in pandas
- append and merge datasets using different join types
- filter and manipulate data to create new variables.

### Aggregation

```
import pandas as pd

data = {'Category': ['A', 'B', 'A', 'B', 'A', 'B'],
        'Value': [10, 20, 15, 25, 12, 18]}

df = pd.DataFrame(data)

grouped = df.groupby('Category')
mean_values = grouped.mean()
print(mean_values)

sum_values = grouped.sum()
print(sum_values)

count_values = grouped.count()
print(count_values)
```

In Pandas, the groupby function is used to group data based on some criteria, and then you can perform various operations on each group.

Another feature is to create a pivot table:

```
# Create a pivot table to show the average age for each gender and country
pivot_table = pd.pivot_table(df, values='Value', index=['Category'], aggfunc='mean')
```

In this example, the pd.pivot_table function is used to create a pivot table. The values parameter specifies the column to aggregate (in this case, 'Value'). The index parameter specifies the rows, and the columns parameter specifies the columns of the pivot table. The aggfunc parameter specifies the aggregation function, which is the mean in this case.

Output:

```
           Value
Category       
A            12.333333
B            21.000000
```

### Append & Merge

The first merge is when you want to add two data sets together that have the same column names. For example person_dataset_1.csv and person_dataset.csv:

```
import pandas as pd

# Read the CSV files into dataframes
df1 = pd.read_csv("person_split1.csv")
df2 = pd.read_csv("person_split2.csv")

# Concatenate the dataframes
df = pd.concat([df1, df2], ignore_index=True)
```

What happens if you relational data, the column names are different but related. For example in the one csv file we have : "id,	Company, Name,	Department,	Job, Title,	Skill" as column names and the other: "id,	University". If you are familiar with SQL databases then you should know about this.

Let us we want to merge these two datasets together. They are related by the "id" column. We can do that with the following code:

```
df1 = pd.read_csv('person_education.csv')
df2 = pd.read_csv('person_work.csv')

# # inner join
df_merge = pd.merge(df1,df2,on='id')
```

So in this case we used the pandas merge method to join them both based on their "id" column. Here, by default an inner join was used.

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html
 
![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/ec912d95-105b-4f6a-93d5-6078cdcac281)


An inner join returns only the rows where there is a match in both dataframes on the specified "on" column (in this case, the "id" column). If there is no match, the row is excluded from the result.

For example, here an outer join is used

```
df_merge = pd.merge(df1, df2, on='id', how='outer')
```

An outer join returns all the rows from both dataframes. If there is no match for a row in either dataframe, 
the missing values will be filled with NaNs. Left and Right Joins are possible too.

### Filtering

We saw in the first lesson using the "country_data.csv" we can fitler simply with the following:

```
# Filtering data
print(df[df['age'] > 30])
```

In a similar way you can do the following:

```
# Filter data for females (Gender == 'F')
female_data = df[df['Gender'] == 'F']

# Calculate the average age of females
average_age_female = female_data['Age'].mean()
```

If you have your own custom change you want to do to each value:

```
import pandas as pd

# Create a sample DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 22],
        'Salary': [50000, 60000, 45000]}

df = pd.DataFrame(data)

# Apply the addition of 5000 directly to the 'Salary' column using a lambda function
df['Salary_with_bonus'] = df['Salary'].apply(lambda x: x + 5000)

# Display the updated DataFrame
print(df)

```

The .apply(lambda x: x + 5000) part is used to apply a function to each element in the selected 'Salary' column. In this case, a lambda function is used. The lambda function takes an input parameter x (each individual salary value) and adds 5000 to it.

The result of the apply operation is a new pandas series or columns with the updated values.

## Summary: Data Cleaning and Preprocessing

Objective: Focus on cleaning and preprocessing extracted data for analysis.

Practical Task: Use pandas for tasks like handling missing values, removing duplicates, and transforming data types. Discuss the importance of data quality.

The most common mistake with spreadsheets is that we treat them like lab notebooks, that is relying on context, side notes, margins, spacial layouts and fields and metadata. We can usually interpret thiese things but computers don't view information in the same way. We need to explain to the computer what each step means. It can't firgure out how data is suppose to fit together.

What is data cleaning and preprocessing. Making it computer ready so it can analyzed. 

What are common things we need to look out for:

1. Not filling in zeros - different to blank, a zero is actual data that was measured
2. Null Values - different to zero, null was not measured and thus should be ignored 
3. Formatting to make data sheet pretty - highlighting and similar - add a new column instead with info
4. Comments in cells - place in separate column
5. Entering more than one piece of information in a cell - only one piece of information per cell
6. Using problematic field names - avoid spaces, numbers, and special characeters
7. Using special characters in data - avoid in your data

# Load:

It is always useful to export the data after you have cleaned and performed the transformations. This is simple done in various formats:

## CSV

```
df.to_csv("country_data_cleaned.csv")
```

If you don't want the Pandas index column you can specify:

```
df.to_csv("country_data_cleaned.csv", index=False)
```

## Excel

```
df.to_excel("country_data_cleaned.xlsx", index=False, sheet_name='Sheet1')
```

## JSON

```
df.to_json("country_data_cleaned.json", orient='records')
```

In JSON format, there isn't a concept of a DataFrame index like in tabular data. The orient='records' argument specifies that the JSON file should be structured as a list of records, and the DataFrame index is not considered.

## Advanced Applications

For advanced users you can learn how to automate ETL processes for efficiency. You can use tools like Apache Airflow or create simple Python scripts to automate ETL pipelines. Schedule tasks to run at specified intervals. You can also use logging to track the progress of an ETL pipeline and handle exceptions.

---

Well done if you have gotten this far. Remember to try all these examples yourself.

>_Start thinking of how you can use your own data in Python_


