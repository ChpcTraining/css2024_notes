# Extract

## Different File Locations

### Local files

```
import pandas as pd

df = pd.read_csv("country_data.csv")

```

### Files in a different location

```
import pandas as pd

df = pd.read_csv("data/country_data.csv")

```

### File from a URL

```
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/ChpcTraining/css2024_data/main/country_data.csv")

```

## Different File Types

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/7fbe4893-264e-4140-8d65-81e215fb9d68)


https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html

### Text Files files

Text file with semi-colon

```
df = pd.read_csv("Geospatial Data.txt",sep=";")
```

### Excel

```
df = pd.read_excel("residentdoctors.xlsx")
```

### Json

```
df = pd.read_json("student_data.json")
```

## Other options:

>_Would you like to see the following:_

- Webscraping
  
- Connecting to Databases


# Transform:

As we saw in the previous lesson, sometimes the data is not in the correct format. Now, if you have a one or two files then just rather do it in Excel itself. If you have many files, rather use Python as you can automate the process and most importanly your script records what you did!

There is a something to consider here is that if you don't know your data is in the wrong format or messy, then some of what you will learn now might not make sense. For example, someone might see your table is messy full of books and they recommend you tidy it up so you can work better. And you say, well its not messy at all I work just fine. Fine maybe, but maybe not as efficient and effective as possible. And if your using simulation software or doing cloud computing your data needs to be unambigious.  

So, what I am saying is, if you your use cleaning your data in Excel or other tools then this should be quite familiar to you. Now we are just using Python to do it.

So there are many ways to clean or transform files. We will begin by getting familiar with some easy built-in operations or parameters that the the the read_csv function has: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/f8168c97-cd0e-47e5-82f9-ef8636016bce)

Let us look at a few files with some small issues:

## Index Column

```
df = pd.read_csv("country_data.csv")
```

The "Unnamed: 0" column often appears in pandas DataFrames when reading a CSV file if the CSV file itself has an index column that was not explicitly specified.

When you read a CSV file into a DataFrame using pd.read_csv(), pandas assigns an index to each row by default. If the CSV file already contains an index column, pandas will create a new index unless you specify the existing column as the index. If no index is specified, pandas will create a default numerical index, and this default index is labeled as "Unnamed: 0" in the DataFrame.

To avoid the appearance of the "Unnamed: 0" column, you can either specify the existing index column from your CSV file when reading the data, or you can use the index_col parameter to explicitly specify which column you want to use as the index.

```
df = pd.read_csv("country_data.csv",index_col=0)
```

This will read the CSV file and use the first column as the index, preventing the appearance of the "Unnamed: 0" column in your DataFrame.

## Skip Rows

```
df = pd.read_csv("insurance_data.csv")
```

We get an error because the number of columns in your CSV file is not consistent across all rows. The read_csv function in pandas assumes a consistent number of columns, but your data seems to have two values in some rows, causing the error.

We can resolve this by using the "skiprows" parameter to just skip first 5 rows:

```
df = pd.read_csv("insurance_data.csv",skiprows=5)
```

## Column Headings

```
df = pd.read_csv("patient_data.csv")
```

No headings :(

Create a list with the heading names you want and add use the parameters "header=None" and "names=column_names":

```
column_names = ["duration", "pulse", "max_pulse", "calories"]

df = pd.read_csv("patient_data.csv", header=None, names=column_names)
```

## Unique Delimiter

```
df = pd.read_csv("Geospatial Data.txt")
```

This is a text file with the data separate with a semi-colon, Pandas is expecting a comma. So we use the parameter sep=";"

```
df = pd.read_csv("data/Geospatial Data.txt",sep=";")
```

Also note that the file name has a space in it, which is not recommended.

##  Inconsistent Data Types & Names

```
df = pd.read_excel("residentdoctors.xlsx")
```

- Some column headings are snake case and other all caps
- AGEDIST has numbers with text
- MARITULSTATUS has text fields

Note with Excel files, you cannot have it open in Spyder and in Excel too.

Let us first look at the different groupings:

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/aebe6e95-62ef-4ebb-bdb7-f251356d7109)

One way to make this more efficient is to convert it the lower age in grouping. So instead of 30-34yrs it would be 34. This is just one way. You can keep it as is however it will prevent you from doing further numerical analysis on it.

Add the following code:

```
# Step 1: Extract the lower end of the age range (digits only)
df['LOWER_AGE'] = df['AGEDIST'].str.extract('(\d+)-')
```

Pseudo-code:
1. Search for a number followed by a hyphen like "30-"
2. If you find that number, extract the number and ignore the hyphen
3. Put it in a new column called LOWER_AGE 

Then convert that number to from s tring to integer, a whole number.

This step uses "str.extract()" to capture one or more digits from the 'AGEDIST' column. When you use `df['AGEDIST'].str.extract('(\d+)-')`, it applies this regular expression pattern to each element in the 'AGEDIST' column. 

Regular expressions (regex or regexp) are sequences of characters that define a search pattern. It extracts the portion of the string that matches the pattern, which is the one or more consecutive digits before the hyphen in the age range. We won't go over regular expressions much, this was just a demonstration to show you different possibilitles.

```
# Step 2: Convert the new column to float
df['LOWER_AGE'] = df['LOWER_AGE'].astype(int)
```

Pseudo-code:
1. Convert all the data to integers
2. Store that value back into LOWER_AGE column

After extracting the digits, this step converts the resulting column to floating-point numbers using astype(float). Without this conversion, the extracted values would be treated as strings, and any subsequent numeric operations or analysis would not work as expected.

#### Pandas Functions/Methods

In pandas, .str, .extract(), and .astype() are all functions or methods that can be applied to a pandas Series object or single column of text data. 

These methods are part of the powerful functionality provided by pandas for manipulating and analyzing data in a DataFrame. 

When you apply .str to a column, it allows you to perform various string operations on each element of the Series. Here we used .extract() to extract certain information from a string. Other string methods:
- .upper() makes string/text upper case
- .lower() makes string/text lower case
- .replace() replaces certain characters with another
  
*Note, they all have brackets as they can accept parameters.*

## Working with Dates

Again there are many ways to do things, sometimes the recommendations are useful and sometimes they are not. Working with dates and time is a tricky subject, as it all depends what you want to do with it. One general problem with dates in Excel is that formatting can be completely different from one laptop to another, you may send an Excel spreadsheet to someone else and the dates maybe completely different which may effect the analysis you did. For example:

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/f6819af8-d970-4b51-9bcb-db272166fe97)

Ideally you would want to work with a date format that is completely clear for yourself and the next person working with it. Let us look at this time series data:

```
df = pd.read_csv("time_series_data.csv")
```

You will notice the date is in this format: 2020-01-10, however in the US, the day is the middle number and in UK and in most local countries here the middle number is the month - as it should be :)



## Applying Data Transformations

Objective: Introduce transformations such as aggregations, merging, and filtering.

Practical Task: Demonstrate how to aggregate data using groupby in pandas. Merge datasets using different join types. Filter and manipulate data to create new variables.

- merge files

- join files 


## Implementing Advanced Transformations

Objective: Teach advanced transformations like pivot tables and custom functions.

Practical Task: Create a pivot table using pandas. Write custom functions to transform data based on specific requirements.

## Summary: Data Cleaning and Preprocessing

Objective: Focus on cleaning and preprocessing extracted data for analysis.

Practical Task: Use pandas for tasks like handling missing values, removing duplicates, and transforming data types. Discuss the importance of data quality.

The most common mistake with spreadsheets is that we treat them like lab notebooks, that is relying on context, side notes, margins, spacial layouts and fields and metadata. We can usually interpret thiese things but computers don't view information in the same way. We need to explain to the computer what each step means. It can't firgure out how data is suppose to fit together.

What is data cleaning and preprocessing. Making it computer ready so it can analyzed. 

What are common things we need to look out for:

1. Not filling in zeros - different to blank, a zero is actual data that was measured
2. Null Values - different to zero, null was not measured and thus should be ignored 
3. Formatting to make data sheet pretty - highlighting and similar - add a new column instead with info
4. Comments in cells - place in separate column
5. Entering more than one piece of information in a cell - only one piece of information per cell
6. Using problematic field names - avoid spaces, numbers, and special characeters
7. Using special characters in data - avoid in your data

# Load:

## Lesson 7: Writing to Files and Databases

Objective: Understand how to load transformed data back into different storage systems.

Practical Task: Write transformed data to a CSV file, Excel file, and a PostgreSQL database. Discuss the benefits and considerations for each storage method.


## Lesson 8: Automating ETL Pipelines

Objective: Learn how to automate ETL processes for efficiency.

Practical Task: Use tools like Apache Airflow or create simple Python scripts to automate ETL pipelines. Schedule tasks to run at specified intervals.

## Lesson 9: Monitoring and Error Handling

Objective: Emphasize the importance of monitoring and handling errors in ETL processes.

Practical Task: Implement error handling mechanisms in a Python script. Use logging to track the progress of an ETL pipeline and handle exceptions.
Overall Project:


