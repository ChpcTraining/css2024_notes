# Extract:

## Extracting Data from Files

Objective: Understand how to extract data from different file formats using Python.

Practical Task: Extract data from a file using the pandas library. 

- local files

```
import pandas as pd

df = pd.read_csv("country_data.csv")

```

- files in different location

```
import pandas as pd

df = pd.read_csv("data/country_data.csv")

```

- file from URL

```
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/ChpcTraining/css2024_data/main/country_data.csv")

```

  
- Different types of files: Excel, JSON, text, CSV


```
import pandas as pd


# df = pd.read_csv("data/country_data.csv")
# index issue - , index_col=0

# df = pd.read_csv("data/GenoType Dataset.csv")
# ???

# df = pd.read_csv("data/Geospatial Data.txt",sep=";")
# delim issue, semi colon
# name issue

# df = pd.read_csv("data/pulse_data.csv")
# nan issue

# df = pd.read_excel("data/residentdoctors.xlsx")
# column name not consistent, spaces
# Age-dist has years in it
# nan

#df = pd.read_json("data/student_data.json")
# how to read in subfields -> transform -> EDA

df = pd.read_csv("data/student-mat.csv")
# pretty clean

# df = pd.read_csv("data/time_series_data.csv")
# first column unamed_0
# df = pd.read_csv('data/time_series_data.csv', index_col=0)


# Display the DataFrame
print(df)
```

## Other options:

- Webscraping
  
- Connecting to Databases


# Transform:


## Lesson 4: Data Cleaning and Preprocessing

Objective: Focus on cleaning and preprocessing extracted data for analysis.

Practical Task: Use pandas for tasks like handling missing values, removing duplicates, and transforming data types. Discuss the importance of data quality.

The most common mistake with spreadsheets is that we treat them like lab notebooks, that is relying on context, side notes, margins, spacial layouts and fields and metadata. We can usually interpret thiese things but computers don't view information in the same way. We need to explain to the computer what each step means. It can't firgure out how data is suppose to fit together.

What is data cleaning and preprocessing. Making it computer ready so it can analyzed. 

What are common things we need to look out for:

1. Not filling in zeros - different to blank, a zero is actual data that was measured
2. Null Values - different to zero, null was not measured and thus should be ignored 
3. Formatting to make data sheet pretty - highlighting and similar - add a new column instead with info
4. Comments in cells - place in separate column
5. Entering more than one piece of information in a cell - only one piece of information per cell
6. Using problematic field names - avoid spaces, numbers, and special characeters
7. Using special characters in data - avoid in your data

Let us go back to a few of those files that we saw before and see what needs to be cleaned up and how.

Let us start with:

```
df = pd.read_excel("data/residentdoctors.xlsx")
# column name not consistent, spaces
# Age-dist has years in it
# nan
```


## Lesson 5: Applying Data Transformations

Objective: Introduce transformations such as aggregations, merging, and filtering.

Practical Task: Demonstrate how to aggregate data using groupby in pandas. Merge datasets using different join types. Filter and manipulate data to create new variables.

- merge files

- join files 


## Lesson 6: Implementing Advanced Transformations

Objective: Teach advanced transformations like pivot tables and custom functions.

Practical Task: Create a pivot table using pandas. Write custom functions to transform data based on specific requirements.

# Load:

## Lesson 7: Writing to Files and Databases

Objective: Understand how to load transformed data back into different storage systems.

Practical Task: Write transformed data to a CSV file, Excel file, and a PostgreSQL database. Discuss the benefits and considerations for each storage method.


## Lesson 8: Automating ETL Pipelines

Objective: Learn how to automate ETL processes for efficiency.

Practical Task: Use tools like Apache Airflow or create simple Python scripts to automate ETL pipelines. Schedule tasks to run at specified intervals.

## Lesson 9: Monitoring and Error Handling

Objective: Emphasize the importance of monitoring and handling errors in ETL processes.

Practical Task: Implement error handling mechanisms in a Python script. Use logging to track the progress of an ETL pipeline and handle exceptions.
Overall Project:

## Lesson 10: Final Project

Objective: Apply all the ETL concepts learned in a comprehensive project.

Practical Task: Design and implement an end-to-end ETL pipeline for a real-world dataset. Include extraction from different sources, thorough data transformations, and loading into a database or file. Present and discuss the project in class.
Remember to encourage collaboration, provide opportunities for questions and problem-solving, and emphasize best practices throughout the lessons.
