# Extract:

## Different File Locations

### Local files

```
import pandas as pd

df = pd.read_csv("country_data.csv")

```

### Files in a different location

```
import pandas as pd

df = pd.read_csv("data/country_data.csv")

```

### File from a URL

```
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/ChpcTraining/css2024_data/main/country_data.csv")

```

## Different File Types

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/7fbe4893-264e-4140-8d65-81e215fb9d68)


https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html

### Text Files files

Text file with semi-colon

```
df = pd.read_csv("Geospatial Data.txt",sep=";")
```

### Excel

```
df = pd.read_excel("residentdoctors.xlsx")
```

### Json

```
df = pd.read_json("student_data.json")
```

## Other options:

>_Would you like to see the following:_

- Webscraping
  
- Connecting to Databases


# Transform:

As we saw in the previous lesson, sometimes the data is not in the correct format. Now, if you have a one or two files then just rather do it in Excel itself. If you have many files, rather use Python as you can automate the process and most importanly your script records what you did!

There are many ways to transform files. We will begin by getting familiar with some easy built-in ways the the read_csv function has: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

![image](https://github.com/ChpcTraining/css2024_notes/assets/157092105/f8168c97-cd0e-47e5-82f9-ef8636016bce)

Let us look at a few files with problems:

## Index Column

```
df = pd.read_csv("country_data.csv")
```

The "Unnamed: 0" column often appears in pandas DataFrames when reading a CSV file if the CSV file itself has an index column that was not explicitly specified.

When you read a CSV file into a DataFrame using pd.read_csv(), pandas assigns an index to each row by default. If the CSV file already contains an index column, pandas will create a new index unless you specify the existing column as the index. If no index is specified, pandas will create a default numerical index, and this default index is labeled as "Unnamed: 0" in the DataFrame.

To avoid the appearance of the "Unnamed: 0" column, you can either specify the existing index column from your CSV file when reading the data, or you can use the index_col parameter to explicitly specify which column you want to use as the index.

```
df = pd.read_csv("country_data.csv",index_col=0)
```

This will read the CSV file and use the first column as the index, preventing the appearance of the "Unnamed: 0" column in your DataFrame.

## Skip Rows

```
df = pd.read_csv("insurance_data.csv")
```

We get an error because the number of columns in your CSV file is not consistent across all rows. The read_csv function in pandas assumes a consistent number of columns, but your data seems to have two values in some rows, causing the error.

We can resolve this by using the "skiprows" parameter to just skip first 5 rows:

```
df = pd.read_csv("insurance_data.csv",skiprows=5)
```

## Skip Rows

df = pd.read_csv("housing_data.csv")



df = pd.read_csv("GenoType Dataset.csv")

df = pd.read_csv("Geospatial Data.txt")

df = pd.read_csv("pulse_data.csv")

df = pd.read_excel("residentdoctors.xlsx")

df = pd.read_csv("time_series_data.csv")
```

## Lesson 4: Data Cleaning and Preprocessing

Objective: Focus on cleaning and preprocessing extracted data for analysis.

Practical Task: Use pandas for tasks like handling missing values, removing duplicates, and transforming data types. Discuss the importance of data quality.

The most common mistake with spreadsheets is that we treat them like lab notebooks, that is relying on context, side notes, margins, spacial layouts and fields and metadata. We can usually interpret thiese things but computers don't view information in the same way. We need to explain to the computer what each step means. It can't firgure out how data is suppose to fit together.

What is data cleaning and preprocessing. Making it computer ready so it can analyzed. 

What are common things we need to look out for:

1. Not filling in zeros - different to blank, a zero is actual data that was measured
2. Null Values - different to zero, null was not measured and thus should be ignored 
3. Formatting to make data sheet pretty - highlighting and similar - add a new column instead with info
4. Comments in cells - place in separate column
5. Entering more than one piece of information in a cell - only one piece of information per cell
6. Using problematic field names - avoid spaces, numbers, and special characeters
7. Using special characters in data - avoid in your data

Let us go back to a few of those files that we saw before and see what needs to be cleaned up and how.

Let us start with:

```
df = pd.read_excel("data/residentdoctors.xlsx")
# column name not consistent, spaces
# Age-dist has years in it
# nan
```


## Lesson 5: Applying Data Transformations

Objective: Introduce transformations such as aggregations, merging, and filtering.

Practical Task: Demonstrate how to aggregate data using groupby in pandas. Merge datasets using different join types. Filter and manipulate data to create new variables.

- merge files

- join files 


## Lesson 6: Implementing Advanced Transformations

Objective: Teach advanced transformations like pivot tables and custom functions.

Practical Task: Create a pivot table using pandas. Write custom functions to transform data based on specific requirements.

# Load:

## Lesson 7: Writing to Files and Databases

Objective: Understand how to load transformed data back into different storage systems.

Practical Task: Write transformed data to a CSV file, Excel file, and a PostgreSQL database. Discuss the benefits and considerations for each storage method.


## Lesson 8: Automating ETL Pipelines

Objective: Learn how to automate ETL processes for efficiency.

Practical Task: Use tools like Apache Airflow or create simple Python scripts to automate ETL pipelines. Schedule tasks to run at specified intervals.

## Lesson 9: Monitoring and Error Handling

Objective: Emphasize the importance of monitoring and handling errors in ETL processes.

Practical Task: Implement error handling mechanisms in a Python script. Use logging to track the progress of an ETL pipeline and handle exceptions.
Overall Project:

## Lesson 10: Final Project

Objective: Apply all the ETL concepts learned in a comprehensive project.

Practical Task: Design and implement an end-to-end ETL pipeline for a real-world dataset. Include extraction from different sources, thorough data transformations, and loading into a database or file. Present and discuss the project in class.
Remember to encourage collaboration, provide opportunities for questions and problem-solving, and emphasize best practices throughout the lessons.
